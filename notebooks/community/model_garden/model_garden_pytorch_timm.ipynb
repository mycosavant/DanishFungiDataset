{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApNHZJmT2AMH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfxrFG052AMI"
      },
      "source": [
        "# Vertex AI Model Garden - TIMM\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_timm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_timm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_timm.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "    (a Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76BCoQcm2AMJ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running local inference using the [timm](https://github.com/rwightman/pytorch-image-models) library, finetuning the PyTorch [timm models](https://github.com/huggingface/pytorch-image-models#models), and deploying the models on [Vertex AI](https://cloud.google.com/vertex-ai).\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Setup environment.\n",
        "- Run inference locally using the timm library.\n",
        "- Create a custom training job on Vertex AI to train or finetune a model.\n",
        "- Deploy the model on Vertex AI for online prediction.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iU1NKfh2AMJ"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l3GN-QL2AMJ"
      },
      "source": [
        "### Setup cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project). Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "1. [Enable Artifact Registry](https://cloud.google.com/artifact-registry/docs/enable-service) and [create a repository](https://cloud.google.com/artifact-registry/docs/repositories/create-repos) for storing docker images.\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyAQXmgf2AMK"
      },
      "source": [
        "### Setup required libraries\n",
        "\n",
        "It's highly recommended to run this notebook on [Vertex AI workbench](https://cloud.google.com/vertex-ai-workbench), where you don't need to manually install any additional libraries.\n",
        "\n",
        "If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk) and [gsutil](https://cloud.google.com/storage/docs/gsutil_install)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad1f36f4b5c3"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cb041d87f50a",
        "outputId": "d18772b4-23f2-41cc-8e9b-d4f808fa4cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uF7Kb112AMK"
      },
      "source": [
        "### Colab Only\n",
        "Run the following commands for colab and skip this section if you use workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2lP6dnfy2AMK",
        "outputId": "9ed30f8d-4899-4195-83b8-3bdac8ceea20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.28.1-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.2-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.56.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: shapely, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed google-cloud-aiplatform-1.28.1 google-cloud-resource-manager-1.10.2 shapely-1.8.5.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n",
        "\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo65Wg9Y2AMK"
      },
      "source": [
        "### Setup environment variables\n",
        "\n",
        "This notebook supports models in https://huggingface.co/docs/timm/models.\n",
        "\n",
        "You can also run\n",
        "`python -c \"from timm import list_models; print(list_models(pretrained=True))\"`\n",
        "locally to see all pretrained models.\n",
        "\n",
        "The following models have been manually verified to work with this notebook:\n",
        "\n",
        "* vit_tiny_patch16_224\n",
        "* beit_base_patch16_224\n",
        "* deit3_small_patch16_224\n",
        "* efficientnet_b2\n",
        "* mobilenetv2_100\n",
        "* resnet50\n",
        "* resnest50d\n",
        "* convnext_base\n",
        "* cspdarknet53\n",
        "* inception_v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3msH2i5V2AMK"
      },
      "outputs": [],
      "source": [
        "# The cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "# The region for running jobs.\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# The model you want to train and serve. Please select a model from the verified model list above.\n",
        "# We use a ViT model as the example.\n",
        "MODEL_NAME = \"vit_tiny_patch16_224\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket name without gs:// prefix for training outputs.\n",
        "# For example: test_bucket\n",
        "GCS_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The service account for deploying fine tuned model. It looks like:\n",
        "# '<account_name>@<project>.iam.gserviceaccount.com'\n",
        "# Follow step 6 above to create this account.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a500024b7b"
      },
      "source": [
        "## Run local inference\n",
        "\n",
        "This section runs local inference on an image using the model chosen in above section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54859170c1ad"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7505a2bf3897"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "from PIL import Image\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e6d8a0dac3"
      },
      "source": [
        "### Load a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b6bbe928998"
      },
      "outputs": [],
      "source": [
        "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2ce8fa5439"
      },
      "source": [
        "### Load and preprocess the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6390302c9761"
      },
      "outputs": [],
      "source": [
        "config = resolve_data_config({}, model=model)\n",
        "transform = create_transform(**config)\n",
        "\n",
        "# The example downloads a test image. You can upload and use your own images\n",
        "# by changing IMAGE_FILENAME.\n",
        "! wget https://github.com/pytorch/hub/raw/master/images/dog.jpg -O test.jpg\n",
        "IMAGE_FILENAME = \"test.jpg\"  # @param {type:\"string\"}\n",
        "\n",
        "# You can also copy over images stored in a GCS bucket with the line below.\n",
        "# ! gsutil cp \"gs://path/to/image\" \"test.jpg\"\n",
        "\n",
        "img = Image.open(IMAGE_FILENAME).convert(\"RGB\")\n",
        "tensor = transform(img).unsqueeze(0)  # transform and add batch dimension\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a6e2ea460ad"
      },
      "source": [
        "### Get the model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a7a666a0aef"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    out = model(tensor)\n",
        "probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "print(probabilities.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "691139fc5789"
      },
      "source": [
        "### Get the top-5 predictions class names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147249af5dec"
      },
      "outputs": [],
      "source": [
        "# Get imagenet class mappings\n",
        "url, filename = (\n",
        "    \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\",\n",
        "    \"imagenet_classes.txt\",\n",
        ")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "with open(\"imagenet_classes.txt\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8740b668da6"
      },
      "source": [
        "### Print top categories per image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f18bcd806f2"
      },
      "outputs": [],
      "source": [
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "for i in range(top5_prob.size(0)):\n",
        "    print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "# prints class names and probabilities like:\n",
        "# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4120d1585355"
      },
      "source": [
        "## Run training jobs\n",
        "\n",
        "This section runs a regular training job or a hyperparameter tuning job on Vertex AI.\n",
        "\n",
        "Before creating a training job, you need to prepare the dataset for training and evaluation.\n",
        "\n",
        "For example, you can use [ImageNet-1K](https://huggingface.co/datasets/imagenet-1k) held on a Cloud Storage bucket as the input dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy4gKksX2AMK"
      },
      "outputs": [],
      "source": [
        "# The prebuilt training docker uri.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-timm-train\"\n",
        "\n",
        "# The path to data directory on Cloud Storage without gs:// prefix.\n",
        "# In the form of: <bucket-name>/path-to-data\n",
        "GCS_DATA_DIR = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DnlJhO72AMK"
      },
      "source": [
        "### Create a training job on Vertex AI\n",
        "\n",
        "This section creates a training job on Vertex AI. If you want to create a hyperparameter tuning job instead, you can skip to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R5JWJGS2AML"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Init common setup.\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)\n",
        "\n",
        "# Input and output path.\n",
        "data_dir = f\"/gcs/{GCS_DATA_DIR}\"\n",
        "output_dir = f\"/gcs/{GCS_BUCKET}/timm\"\n",
        "\n",
        "# Worker pool spec.\n",
        "# Single node with multiple GPUs.\n",
        "machine_type = \"n1-highmem-32\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_P100\"  # @param {type:\"string\"}\n",
        "num_gpus = 4  # @param {type:\"integer\"}\n",
        "\n",
        "# Model specific config.\n",
        "job_name = f\"pytorch-{MODEL_NAME}\"\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--standalone\",\n",
        "        f\"--nnodes={num_nodes}\",\n",
        "        f\"--nproc_per_node={num_gpus}\",\n",
        "        \"train.py\",\n",
        "        data_dir,\n",
        "        f\"--model={MODEL_NAME}\",\n",
        "        \"--pretrained\",\n",
        "        f\"--output={output_dir}\",\n",
        "        f\"--batch-size={batch_size}\",\n",
        "        f\"--epochs={epochs}\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf6P7ZI82AML"
      },
      "source": [
        "### Create a hyperparameter tuning job on Vertex AI\n",
        "\n",
        "You can use a [hyperparameter tuning](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) job to find the best configuration of your hyperparameters.\n",
        "\n",
        "You can skip this section if you already trained a model in the previous section and do not want to tune the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy_aCff_2AML"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "# Init common setup.\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)\n",
        "\n",
        "# Input and output path.\n",
        "data_dir = f\"/gcs/{GCS_DATA_DIR}\"\n",
        "output_dir = f\"/gcs/{GCS_BUCKET}/timm\"\n",
        "\n",
        "# Model specific config.\n",
        "job_name = f\"pytorch-hp-{MODEL_NAME}\"\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-16\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"  # @param {type:\"string\"}\n",
        "num_gpus = 2  # @param {type:\"integer\"}\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": gpu_type,\n",
        "            \"accelerator_count\": num_gpus,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"--standalone\",\n",
        "                f\"--nnodes={num_nodes}\",\n",
        "                f\"--nproc_per_node={num_gpus}\",\n",
        "                \"train.py\",\n",
        "                data_dir,\n",
        "                f\"--model={MODEL_NAME}\",\n",
        "                \"--pretrained\",\n",
        "                f\"--output={output_dir}\",\n",
        "                f\"--batch-size={batch_size}\",\n",
        "                f\"--epochs={epochs}\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Hyperparameter job specs.\n",
        "metric_spec = {\"top1_accuracy\": \"maximize\"}\n",
        "parameter_spec = {\n",
        "    \"lr\": hpt.DoubleParameterSpec(min=0.001, max=0.05, scale=\"log\"),\n",
        "}\n",
        "max_trial_count = 2\n",
        "parallel_trial_count = 2\n",
        "\n",
        "# Launch jobs.\n",
        "training_job = aiplatform.CustomJob(\n",
        "    display_name=job_name, worker_pool_specs=worker_pool_specs\n",
        ")\n",
        "hp_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=job_name,\n",
        "    custom_job=training_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=max_trial_count,\n",
        "    parallel_trial_count=parallel_trial_count,\n",
        ")\n",
        "hp_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyRWwqW2AML"
      },
      "source": [
        "## Deploy model for online prediction\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on an Endpoint resource.\n",
        "\n",
        "The model deployment step will take ~15 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbNbg0yR2AML"
      },
      "outputs": [],
      "source": [
        "# The prebuilt serving docker uri.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-timm-serve\"\n",
        "# The port number used by torchserve traffic.\n",
        "SERVE_PORT = 7080\n",
        "# The path to model checkpoint file, including gs:// prefix.\n",
        "MODEL_PT_PATH = \"gs://path_to_model_best.pth.tar\"  # @param {type:\"string\"}\n",
        "# [Optional] the path to index_to_name.json, including gs:// prefix.\n",
        "INDEX_TO_NAME_FILE = \"gs://path_to_index_to_name.json\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INPri3HQ2AML"
      },
      "source": [
        "### Upload and deploy model on Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOvqoAaN2AML"
      },
      "outputs": [],
      "source": [
        "# Upload model.\n",
        "serving_env = {\n",
        "    \"MODEL_NAME\": MODEL_NAME,\n",
        "    \"MODEL_PT_PATH\": MODEL_PT_PATH,\n",
        "    \"INDEX_TO_NAME_FILE\": INDEX_TO_NAME_FILE,\n",
        "}\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_NAME,\n",
        "    serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "    serving_container_ports=[SERVE_PORT],\n",
        "    serving_container_predict_route=\"/predictions/timm_serving\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables=serving_env,\n",
        ")\n",
        "# Or reuse a pre-uploaded model.\n",
        "# model = aiplatform.Model('projects/123456789/locations/us-central1/models/123456789@1')\n",
        "\n",
        "# Create an endpoint.\n",
        "endpoint = aiplatform.Endpoint.create(display_name=\"pytorch-timm-endpoint\")\n",
        "# Or reuse a pre-created endpoint.\n",
        "# endpoint = aiplatform.Endpoint('projects/123456789/locations/us-central1/endpoints/123456789')\n",
        "\n",
        "# Deploy model to endpoint.\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
        "    accelerator_count=1,\n",
        "    traffic_percentage=100,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2gnYFEJ2AML"
      },
      "source": [
        "You can mange your uploaded models in the [Model Registry](https://pantheon.corp.google.com/vertex-ai/models) and your endpoints in the [Endpoints](https://pantheon.corp.google.com/vertex-ai/endpoints)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UP84Q5R2AMM"
      },
      "source": [
        "### Test online prediction\n",
        "\n",
        "You will now test the deployed endpoint. Please prepare an image to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL2Qbm2x2AMM"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# You can get the deployed endpoint object by its resource name returned by Endpoint.create(). For example:\n",
        "# endpoint = aiplatform.Endpoint('projects/816369962409/locations/us-central1/endpoints/8809168414485512192')\n",
        "\n",
        "# Please upload an image and enter its filename below.\n",
        "IMAGE_FILENAME = \"test.jpg\"  # @param {type:\"string\"}\n",
        "\n",
        "# Alternatively, uncomment the following line to download a cat image for demonstration.\n",
        "# ! wget http://images.cocodataset.org/val2017/000000039769.jpg -O test.jpg\n",
        "\n",
        "with open(IMAGE_FILENAME, \"rb\") as f:\n",
        "    image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "instances = [{\"data\": {\"b64\": image_b64}}]\n",
        "\n",
        "prediction = endpoint.predict(instances=instances)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqSxSyT42AMM"
      },
      "source": [
        "### Clean Up Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMwdxiH-2AMM"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy_all()\n",
        "model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_timm.ipynb",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}